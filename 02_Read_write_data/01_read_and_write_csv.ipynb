{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with CSV Files in Spark DataFrame\n",
    "\n",
    "1. **Read a CSV file into a Spark DataFrame**  \n",
    "2. **Read a CSV file with a user-provided schema**  \n",
    "3. **Read multiple CSV files**  \n",
    "4. **Read all CSV files from a directory**  \n",
    "5. **Options while reading a CSV file**  \n",
    "6. **Write a DataFrame to CSV files**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup \n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .master(\"local[3]\")\n",
    "        .appName(\"my_spark_app\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read a CSV file into a Spark DataFrame** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./source/csv/employee_1.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"./source/csv/employee_1.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `spark` is a SparkSession object. `read` is an object of DataFrameReader class and `csv()` is a method in DataFrameReader.  \n",
    "\n",
    "The above example reads the data into DataFrame column names “_c0” for the first column and “_c1” for the second, and so on. By default, the data type of all these columns would be String.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the schema (column_names and Data_type) with help of `printSchema()` method on dataframe  \n",
    "if we didn't specify schema then all columns are string type by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using option `header` as `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load(\"./source/csv/employee_1.csv\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Multipal Csv files  \n",
    "\n",
    "To read multiple CSV files into a PySpark DataFrame, each separated by a comma, you can create a list of file paths and pass it to the spark.read.csv() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load([\"./source/csv/employee_1.csv\", \"./source/csv/employee_2.csv\"]))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read All csv file from folder\n",
    "\n",
    "To read all CSV files from a directory, specify the directory path as an argument to the csv() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load(\"./source/csv/\"))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use wildcard charecter for reading all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load(\"./source/csv/employee_*.csv\"))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading CSV files with different options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"./source/csv/\"))\n",
    "print(df.count())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can chain the option as shown above or you can specify all options at once with `options()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .options(header =True, inferSchema=True)\n",
    "      .load(\"./source/csv/\"))\n",
    "print(df.count())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**most commonly used options are**  \n",
    "\n",
    "| Option       | Description                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|\n",
    "| delimiter    | Specifies the character used to separate fields in the CSV file.           |\n",
    "| inferSchema  | Automatically infers the data types of columns based on the file's content.|\n",
    "| header       | Indicates whether the first row of the CSV contains column names.          |\n",
    "| quotes       | Defines the character used for quoting fields containing special characters.|\n",
    "| nullValues   | Specifies the string that represents null or missing values in the data.   |\n",
    "| dateFormat   | Sets the format for parsing date columns in the file.                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Read dataframe with custom schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"FirstName\", StringType(), True),\n",
    "    StructField(\"LastName\", StringType(), True),\n",
    "    StructField(\"StartDate\", StringType(), True),\n",
    "    StructField(\"ExitDate\", StringType(), True),\n",
    "    StructField(\"Title\", StringType(), True),\n",
    "    StructField(\"Supervisor\", StringType(), True),\n",
    "    StructField(\"ADEmail\", StringType(), True),\n",
    "    StructField(\"BusinessUnit\", StringType(), True),\n",
    "    StructField(\"EmployeeStatus\", StringType(), True),\n",
    "    StructField(\"EmployeeType\", StringType(), True),\n",
    "    StructField(\"PayZone\", StringType(), True),\n",
    "    StructField(\"EmployeeClassificationType\", StringType(), True),\n",
    "    StructField(\"DepartmentType\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    "    StructField(\"DOB\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"JobFunctionDescription\", StringType(), True),\n",
    "    StructField(\"GenderCode\", StringType(), True),\n",
    "    StructField(\"LocationCode\", IntegerType(), True),\n",
    "    StructField(\"RaceDesc\", StringType(), True),\n",
    "    StructField(\"MaritalDesc\", StringType(), True),\n",
    "    StructField(\"Performance_Score\", StringType(), True),\n",
    "    StructField(\"Current_Employee_Rating\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .schema(schema)\n",
    "      .load(\"./source/csv/\"))\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Writing pyspark dataframe to csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write a PySpark DataFrame to a CSV file, you can use the write.csv() method provided by the DataFrame API. This method takes a path as an argument, where the CSV file will be saved. Optionally, you can specify additional parameters such as the delimiter, header inclusion, and whether to overwrite existing files. Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "        .format(\"csv\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\",True)\n",
    "        .option(\"delimiter\",'|')\n",
    "        .save(\"./Sinck/csv/employee\"))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify different saving modes while writing PySpark DataFrame to disk. These saving modes specify how to write a file to disk.\n",
    "\n",
    "`overwrite` – Overwrite the existing file if already exists.\n",
    "\n",
    "`append` – New rows are appended to the existing rows.\n",
    "\n",
    "`ignore` – When this option is used, it ignores the writing operation when the file already exists.\n",
    "\n",
    "`error` – This option returns an error when the file already exists. This is a default option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
